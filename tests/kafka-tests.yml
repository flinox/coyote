#- name: kapacitor
#  title: Kapacitor testes unitarios
# Tests we should add:
# - kafka-avro-console-consumer
# - test hdfs-sink (difficult, test with kerberos in standalone)

- name: Kafka Brokers
  skip: _kafka_brokers_
  entries:
# BASIC / PERFORMANCE
    - name: Create Topic (basic kafka)
      command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test --partition 3 --replication-factor 3 --create
    - name: List Topics (basic kafka)
      command: kafka-topics --zookeeper zookeeper:2181 --list
    - name: Performance Test (basic kafka)
      command: |
        kafka-producer-perf-test --topic kapacitor-test --throughput 100000 --record-size 1000 --num-records 2000000
                                 --producer-props bootstrap.servers="kafka:9092"
      timeout: 90s

- name: Kafka REST Proxy
  skip: _kafka_rest_proxy_
  entries:
# HTTP REST PROXY
    - name: List Topics (rest proxy)
      command: curl -vs --stderr - "http://restproxy:8082/topics"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Topic Information (rest proxy)
      command: curl -vs --stderr - "http://restproxy:8082/topics/kapacitor-test"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Topic Partitions (rest proxy)
      command: curl -vs --stderr - "http://restproxy:8082/topics/kapacitor-test/partitions"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Delete Topic (basic kafka)
      command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test --delete
    - name: Produce Avro Message (rest proxy, schema registry)
      # Please do not change the formatting (e.g. add new lines) of the JSON message below, REST Proxy is very sensitive.
      command: |
        curl -vs --stderr - -XPOST
             -H "Content-Type: application/vnd.kafka.avro.v1+json"
             --data '{"value_schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}",
                      "records": [{"value": {"name": "testUser"}}]}'
             "http://restproxy:8082/topics/kapacitor-test-avro"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Create Consumer for Avro data (rest proxy, schema registry)
      command: |
        curl -vs --stderr - -XPOST -H "Content-Type: application/vnd.kafka.v1+json"
             --data '{"name": "test_consumer", "format": "avro", "auto.offset.reset": "smallest"}'
             "http://restproxy:8082/consumers/kapacitor-avro"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Consume Avro Message (rest proxy, schema registry)
      command: |
        curl -vs --stderr - -XGET -H "Accept: application/vnd.kafka.avro.v1+json"
             "http://restproxy:8082/consumers/kapacitor-avro/instances/test_consumer/topics/kapacitor-test-avro"
      stdout_has: [ 'testUser' ]
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Delete Avro Consumer (rest proxy, schema registry)
      command: curl -vs --stderr - -X DELETE "http://restproxy:8082/consumers/kapacitor-avro/instances/test_consumer"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test-avro --delete
      nolog: true

    - name: Produce JSON Message (rest proxy)
      command: |
        curl -vs --stderr - -XPOST -H "Content-Type: application/vnd.kafka.json.v1+json"
              --data '{"records":[{"value":{"foo":"bar"}}]}' "http://restproxy:8082/topics/kapacitor-test-json"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Create Consumer for JSON data (rest proxy)
      command: |
        curl -vs --stderr - -XPOST -H "Content-Type: application/vnd.kafka.v1+json"
              --data '{"name": "test_consumer", "format": "json", "auto.offset.reset": "smallest"}'
              "http://restproxy:8082/consumers/kapacitor-json"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Consume JSON Message (rest proxy)
      command: |
        curl -vs --stderr - -XGET -H "Accept: application/vnd.kafka.json.v1+json" 
              "http://restproxy:8082/consumers/kapacitor-json/instances/test_consumer/topics/kapacitor-test-json"
      stdout_has: [ 'foo.*bar' ]
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Delete JSON Consumer (rest proxy)
      command: curl -vs --stderr - -X DELETE "http://restproxy:8082/consumers/kapacitor-json/instances/test_consumer"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test-json --delete
      nolog: true

    - name: Produce Binary Message (rest proxy)
      command: |
        curl -vs --stderr - -XPOST -H "Content-Type: application/vnd.kafka.binary.v1+json"
            --data '{"records":[{"value":"S2Fma2E="}]}' "http://restproxy:8082/topics/kapacitor-test-binary"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Create Consumer for Binary data (rest proxy)
      command: |
        curl -vs --stderr - -XPOST -H "Content-Type: application/vnd.kafka.v1+json"
              --data '{"name": "test_consumer", "format": "binary", "auto.offset.reset": "smallest"}'
              "http://restproxy:8082/consumers/kapacitor-binary"
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Consume Binary Message (rest proxy)
      command: |
        curl -vs --stderr - -XGET -H "Accept: application/vnd.kafka.binary.v1+json"
              "http://restproxy:8082/consumers/kapacitor-binary/instances/test_consumer/topics/kapacitor-test-binary"
      stdout_has: [ 'S2Fma2E=' ]
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Delete Binary Consumer (rest proxy)
      command: curl -vs --stderr - -XDELETE "http://restproxy:8082/consumers/kapacitor-binary/instances/test_consumer"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test-binary --delete
      nolog: true

- name: Confluent Schema Registry
  skip: _confluent_schema_registry_
  entries:
# SCHEMA REGISTRY
    - name: Register a new Schema version (schema registry)
      command: |
        curl  -vs --stderr - -XPOST -i -H "Content-Type: application/vnd.schemaregistry.v1+json"
             --data '{"schema": "{\"type\": \"string\"}"}'
             "http://schemaregistry:8081/subjects/kapacitor-test/versions"
    - name: List subjects (schema registry)
      command: curl -vs --stderr - -XGET -i "http://schemaregistry:8081/subjects"
      stdout_has: [ 'kapacitor-test' ]
    - name: List Schema versions (schema registry)
      command: curl -vs --stderr - -XGET -i "http://schemaregistry:8081/subjects/kapacitor-test/versions"
    # - name: Fetch Schema by globally unique id 1 (schema registry)
    #   command: curl -vs --stderr - -XGET -i "http://schemaregistry:8081/schemas/ids/1"
    #   stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Fetch Schema by name and version (schema registry)
      command: curl -vs --stderr - -XGET -i "http://schemaregistry:8081/subjects/kapacitor-test/versions/1"
      stdout_has: [ '"subject":"kapacitor-test","version":1' ]
      stdout_not_has: [ 'error_code":[0-9]', 'Unexpected', 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Register Complex Schema (schema registry)
      command: |
        curl -vs --stderr - -XPOST -i -H "Content-Type: application/vnd.schemaregistry.v1+json"
             --data '{"schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}]}"}'
             "http://schemaregistry:8081/subjects/kapacitor-test-02/versions"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Test Schema Compatibility (schema registry)
      command: |
        curl -vs --stderr - -XPOST -i -H "Content-Type: application/vnd.schemaregistry.v1+json"
             --data '{"schema": "{\"type\": \"record\", \"name\": \"User\", \"fields\": [{\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"address\", \"type\": \"string\"}]}"}'
             "http://schemaregistry:8081/compatibility/subjects/kapacitor-test-02/versions/latest"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdout_has: [ 'is_compatible' ]
    - name: Get Schema Registry Configuration (schema registry)
      command: curl -vs --stderr - -XGET -i "http://schemaregistry:8081/config"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]

- name: Kafka Connect
  skip: _kafka_connect_
  entries:
# CONNECT
    - name: Get list of Connectors (connect distributed)
      command: curl -vs --stderr - -XGET -i "http://connect:8085/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]

    - name: Create a Console Connector (connect distributed)
      command: |
        curl -vs --stderr - -X POST -H "Content-Type: application/json" 
             --data '{ "name": "kapacitor-test-console-source-%UNIQUE_CD%", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector","tasks.max":"1","topic":"kapacitor-connect-distributed-test-%UNIQUE_CD%","file":"/etc/fstab"}}'
             "http://connect:8085/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Sleep a bit to let the connector work
      command: sleep 9
      nolog: true
    - name: Get Connector s Configuration (connect distributed)
      command: curl -vs --stderr - -XGET -i "http://connect:8085/connectors/kapacitor-test-console-source-%UNIQUE_CD%"
      stdout_has: [ '/etc/fstab' ]
    - name: Sleep a bit to let the connector work
      command: sleep 9
      nolog: true
    - name: "Run Console Consumer to fix Kafka's transient state (basic kafka)"
      command: |
        timeout 5
        kafka-console-consumer --new-consumer
                               --bootstrap-server kafka:9092
                               --topic "kapacitor-connect-distributed-test-%UNIQUE_CD%"
                               --from-beginning
      ignore_exit_code: true
    - name: Run Console Consumer (basic kafka)
      command: |
        timeout 5
        kafka-console-consumer --new-consumer
                               --bootstrap-server kafka:9092
                               --topic "kapacitor-connect-distributed-test-%UNIQUE_CD%"
                               --from-beginning
      stdout_has: [ '/etc/fstab' ]
      ignore_exit_code: true
    - name: Delete connector
      command:  curl -vs --stderr - -XDELETE "http://connect:8085/connectors/kapacitor-test-console-source-%UNIQUE_CD%"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
    - name: Delete Connect Distributes Test Topic (basic kafka)
      command: kafka-topics --zookeeper zookeeper:2181 --topic "kapacitor-connect-distributed-test-%UNIQUE_CD%" --delete

    - command: rm -rf kapacitor_test.sqlite kapacitor_sqlite_connector.properties kapacitor_connect_standalone.properties kapacitor_connect.offset
      nolog: true
    - name: Create and Init SQLite database
      command: sqlite3 kapacitor_test.sqlite
      stdin: |
        CREATE TABLE accounts(id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, name VARCHAR(255));
        INSERT INTO accounts(name) VALUES('alice');
        INSERT INTO accounts(name) VALUES('bob');
    - name: Create kapacitor_sqlite_connector.properties
      command: tee kapacitor_sqlite_connector.properties
      stdin: |
        name=kapacitor-test-connect-standalone-%UNIQUE%
        connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
        tasks.max=1
        connection.url=jdbc:sqlite:kapacitor_test.sqlite
        mode=incrementing
        incrementing.column.name=id
        topic.prefix=kapacitor-test-connect-standalone-
    - name: Create kapacitor_connect_standalone.properties
      command: tee kapacitor_connect_standalone.properties
      stdin: |
        bootstrap.servers=kafka:9092
        key.converter=io.confluent.connect.avro.AvroConverter
        key.converter.schema.registry.url=http://schemaregistry:8081
        value.converter=io.confluent.connect.avro.AvroConverter
        value.converter.schema.registry.url=http://schemaregistry:8081
        internal.key.converter=org.apache.kafka.connect.json.JsonConverter
        internal.value.converter=org.apache.kafka.connect.json.JsonConverter
        internal.key.converter.schemas.enable=false
        internal.value.converter.schemas.enable=false
        offset.storage.file.filename=kapacitor_connect.offset
        offset.flush.interval.ms=5000
        zookeeper=zookeeper:2181
        rest.port=38081
        port=38081
        kafka.logs.dir=logs/
    - name: Read SQLite into Topic (connect standalone)
      command: timeout 9 connect-standalone kapacitor_connect_standalone.properties kapacitor_sqlite_connector.properties
      stdout_not_has: [ 'ERROR' ]
      ignore_exit_code: true
    - name: Run Console Consumer (basic kafka)
      command: |
        timeout 5
        kafka-console-consumer --new-consumer
                               --bootstrap-server kafka:9092
                               --topic kapacitor-test-connect-standalone-accounts
                               --from-beginning
      stdout_has: [ 'alice', 'bob' ]
      ignore_exit_code: true
    - name: Delete Connect Standalone Test Topic (basic kafka)
      command: kafka-topics --zookeeper zookeeper:2181 --topic kapacitor-test-connect-standalone-accounts --delete
    - command: rm -rf kapacitor_test.sqlite kapacitor_sqlite_connector.properties kapacitor_connect_standalone.properties kapacitor_connect.offset
      nolog: true
